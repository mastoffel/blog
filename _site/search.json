[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Martin, an independent researcher. I’m currently thinking about the intersection of AI safety and biology funded by an Open Philantropy grant, while building AI agents for a small company on the side. Previously, I led the development of AutoEmulate, an open-source package to emulate physics simulations using machine learning at the Turing Institute; explored parallels between cultural evolution and AI progress as a PIBBSS fellow; studied deleterious mutations in a bronze-age island sheep population; discovered genetic traces of the 19th century overhunting in pinniped populations around the world, consulted on whether racehorse breeding actually works, and built scientific open source software.\nThings I like: trying to write a blog, Pokemon Red, Steel Maces, Open Water Swimming, The Tao of Pooh, cycling against the wind on Snaefellsnes, playing Ukulele with my 1 year old, saying serious things in spontaneous conversations.\nPlease get in touch if you’d like to chat!"
  },
  {
    "objectID": "posts/dacc-bio/index.html",
    "href": "posts/dacc-bio/index.html",
    "title": "d/acc bio",
    "section": "",
    "text": "Biological laboratories are less safe than one might think. Looking at known incidents between 2000 and 2021 roughly every couple of weeks a lab worker accidentally gets infected, and roughly once a year, a pathogen escapes the lab. These are only reported accidents, likely to be an underestimate.\nSome of them are hard to believe. The only human infectious disease that we ever managed to eradicate is smallpox, caused by the variola virus. Since its eradication in 1980, there should just be two high-security stashes of it worldwide, one in the US and one in Russia. Yet, a third stash was discovered in 2014, in a six decade old cardboard box in an unsecured storage room on the NIH campus. Just a year later, the Pentagon accidentally sent live Anthrax samples to various places, including South Korea.\nAnd the list goes on. Brucella leaked from a biopharma plant in Lanzhou, infecting 10k people. The 1977 Russian flu pandemic had signs of a lab leak; the underlying H1N1 flu strain resembled a virus circulating 30 years earlier.\nBiosecurity is simply not that easy, humans make mistakes and accidents happen. But accidents are only part of the risk.\nBioweapons have a long history. From catapulting plague-infected bodies to blankets infected with smallpox, humans have found many ways to weaponise biological agents. Several countries had biological weapons programs, and some are suspected to still run them. Yet, the Biological Weapons Convention, an international treaty with the mission to effectively ban the development of bioweapons, runs with a handful of employees on roughly the budget of an average McDonald’s.\nThen there’s bioterrorism. In 1984, the Rajneeshee sect contaminated salads in restaurants in Oregon with Salmonella, poisening 750 people. Aum Shinrikyo, a Japanese doomsday cult, tried and failed to develop bioweapons but managed to carry out a chemical weapons attack on the Tokyo subway in 1995, killing and injuring many. In 2001, a US scientist sent anthrax letters to various senators and journals, resulting in several casualties and injuries.\nIn the near future, it will become increasingly possible for terrorists to design much more dangerous bio-agents.\nFirst, biotech is becoming increasingly cheap and available. Sequencing a human genome cost 100 million dollars in 2001, now it’s a few hundred. We can order synthesised DNA and get it shipped to us in a week. While most companies do screen orders for potentially dangerous sequences, around 20% don’t. Soon, it might even be possible to just synthesise DNA at home on a small benchtop device. With the right DNA, a real virus can be created using reverse genetics. While this is not that easy, future AI models will be increasingly capable assistants for biotech work, allowing less and less knowledgable actors to create pathogens in the lab.\nSo, the stakes are high. But don’t despair. In the spirit of d/acc, in the next few posts I’ll have a look at biosecurity technologies that will help safeguard humanity against ever more likely biological threats. dis the idea that we should differentially focus on developing defensive, democratic and decentralised technologies. Think early-detection systems and open source vaccines instead of gain-of-function research. Let’s have a look what is out there."
  },
  {
    "objectID": "posts/map/index.html",
    "href": "posts/map/index.html",
    "title": "map()-magic",
    "section": "",
    "text": "“Form follows function” - Louis Sullivan\nOne of the major steps in becoming a more effective R programmer for me was to really adopt functional programming. It made my code more readable, less error-prone, faster, and also simply for fun to write.\nFunctional programming is simply about writing code with functions. Instead of repeating the same line of code over and over or using double-nested for loops, we can abstract the essence of what we are doing into functions.\nA function can then be elegantly applied to many inputs. Here, we will do this with purrr::map(), which I’m using day in and day out and which is a great starting point to dive into the world of functional programming.\nLet’s go through some of the map()-magic with a minimal workflow to produce clean, robust and fast code. We will do a small :genome-wide association study, an analysis looking at the association between genes and a trait by fitting a model over and over again for every :SNP in the genome.\nHere are some my favorite packages.\nlibrary(purrr) # provides the key function here: map\nlibrary(furrr) # does map in parallel\nlibrary(dplyr) # does all sorts of magic\nlibrary(glue)  # concatenates strings beautifully\nlibrary(broom) # takes a model and returns a tidy data.frame\nLet’s see whether drinking coffee has a genetic basis. We make up a trait (coffees per day) and 100 SNPs for 100 individuals."
  },
  {
    "objectID": "posts/map/index.html#simulate-data",
    "href": "posts/map/index.html#simulate-data",
    "title": "map()-magic",
    "section": "Simulate data",
    "text": "Simulate data\n\ncoffees   &lt;- sample(1:6, 100, TRUE)\nsnps      &lt;- replicate(100, sample(c(0,1,2), 100, TRUE))\nsnp_names &lt;- paste0(\"snp\", 1:100)\n\ndat &lt;- data.frame(cbind(coffees, snps)) %&gt;% \n            setNames(c(\"coffees\", snp_names))\n            \nhead(dat[1:5, 1:7])\n\n#&gt;   coffees snp1 snp2 snp3 snp4 snp5 snp6\n#&gt; 1       4    0    1    0    0    2    1\n#&gt; 2       1    2    2    0    0    0    0\n#&gt; 3       4    2    0    1    0    2    0\n#&gt; 4       2    1    2    2    0    1    2\n#&gt; 5       3    1    2    2    1    0    1"
  },
  {
    "objectID": "posts/map/index.html#write-a-function",
    "href": "posts/map/index.html#write-a-function",
    "title": "map()-magic",
    "section": "1) Write a function",
    "text": "1) Write a function\nWe could now do 100 linear models manually by writing 100 lines of code, or we could do a for loop.\nInstead, let’s write a function to fit one model, and then apply this function to each SNP. We generally want the thing that changes (i.e. snp_name) to be the first argument. The function below fits a linear model of coffee consumption with a snp as predictor, and extracts the model estimate and p-value for the SNP. It returns a one-row data.frame. I generally like my functions to return data.frames, because that makes it easy to put together many function outputs into a big data.frame at the end.\n\nfit_model &lt;- function(snp_name, dat) {\n      # write formula using SNP name\n      model_formula &lt;- glue(\"coffees ~ {snp_name}\")\n      # fit linear model\n      fit &lt;- lm(model_formula, data = dat) %&gt;% \n                  broom::tidy() %&gt;%        # tidy results\n                  filter(term == snp_name) # extract snp\n      return(fit)\n      \n}"
  },
  {
    "objectID": "posts/map/index.html#use-map-to-apply-function-to-every-snp",
    "href": "posts/map/index.html#use-map-to-apply-function-to-every-snp",
    "title": "map()-magic",
    "section": "2) Use map() to apply function to every SNP",
    "text": "2) Use map() to apply function to every SNP\nUsing a vector with snp_names, we can apply the function to every SNP. The structure of map() is always the same: map(list/vector, function, additional_arguments). map() always returns a list. We can convert the list to a data.frame with dplyr::bind_rows().\n\n# run gwas\ngwas &lt;- map(snp_names, fit_model, dat) %&gt;% \n              bind_rows()\n# print first three SNPs\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 × 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#what-if-something-goes-wrong---map-safely",
    "href": "posts/map/index.html#what-if-something-goes-wrong---map-safely",
    "title": "map()-magic",
    "section": "3) What if something goes wrong? - map() safely",
    "text": "3) What if something goes wrong? - map() safely\nLoops often fail becomes something goes wrong in one or a few iterations.Let’s introduce a non-existing SNP and try again to see how it fails\n\nsnp_names2 &lt;- c(\"not_a_snp\", snp_names)\n\ngwas &lt;- map(snp_names2, fit_model, dat) %&gt;% \n      bind_rows()\n\n#&gt; Error in `map()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error:\n#&gt; ! object 'not_a_snp' not found\n\n\nWe can make our gwas error-safe using purrr::safely(). This does some magic under the hood which isn’t so important now. For every iteration, it will return a list with two elements, one for the result and one for the error (equals NULL if there is no error). This way, we always get the results or errors of all our iterations back.\n\nfit_model_safely &lt;- purrr::safely(fit_model)\n\ngwas &lt;- map(snp_names2, fit_model_safely, dat)\ngwas[1:2]\n\n#&gt; [[1]]\n#&gt; [[1]]$result\n#&gt; NULL\n#&gt; \n#&gt; [[1]]$error\n#&gt; &lt;simpleError in eval(predvars, data, env): object 'not_a_snp' not found&gt;\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [[2]]$result\n#&gt; # A tibble: 1 × 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1    -0.227     0.211     -1.08   0.285\n#&gt; \n#&gt; [[2]]$error\n#&gt; NULL\n\n\nmap() lets you extract a list element simply by its name. Here, we iterate over the list of results and extract all SNPs that worked.\n\ngwas &lt;- map(gwas, \"result\") %&gt;% \n            bind_rows()\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 × 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#what-if-it-takes-too-long---map-in-parallel",
    "href": "posts/map/index.html#what-if-it-takes-too-long---map-in-parallel",
    "title": "map()-magic",
    "section": "4) What if it takes too long? - map() in parallel",
    "text": "4) What if it takes too long? - map() in parallel\nOnce the idea of map() is clear, we can easily parallelise it to run on multiple cores. There is some overhead in collecting computations from several cores so this doesn’t make much sense when the running time is short. But for longer computations, using 4 cores instead of 1 should make it nearly 4 times as fast.\nWe can use the furrr package here, which mimics purrr functions but can run in parallel. It is based on future, which is why all functions start with future_, for example future_map(). Unlike other ways of parallelising, this approach works on Windows, Mac and Linux. All we have to do now is to first set up a plan() …\n\n# check available cores\navailableCores()\n# parallelises across 4 cores\nplan(multisession, workers = 4)\n\n… and then replace map() with future_map().\n\ngwas &lt;- future_map(snp_names, fit_model, dat) %&gt;% \n            bind_rows()\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 × 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#the-full-minimal-workflow-for-a-robust-parallel-gwas",
    "href": "posts/map/index.html#the-full-minimal-workflow-for-a-robust-parallel-gwas",
    "title": "map()-magic",
    "section": "The full, minimal workflow for a robust, parallel gwas",
    "text": "The full, minimal workflow for a robust, parallel gwas\n\nplan(multisession, workers = 4)\n\nfit_model &lt;- function(snp_name, dat) {\n      model_formula &lt;- glue(\"coffees ~ {snp_name}\")\n      fit &lt;- lm(model_formula, data = dat) %&gt;% \n                  broom::tidy() %&gt;% \n                  filter(term == snp_name)\n      return(fit)\n}\n\nfit_model_safely &lt;- purrr::safely(fit_model)\ngwas &lt;- future_map(snp_names, fit_model_safely, dat) %&gt;% \n                  map(\"result\") %&gt;% \n                  bind_rows()\n\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 × 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#general-thoughts-about-map",
    "href": "posts/map/index.html#general-thoughts-about-map",
    "title": "map()-magic",
    "section": "General thoughts about map",
    "text": "General thoughts about map\nWhen to use map?\n\nWhenever more than two lines of code look similar, whenever a for loop needs two cups of coffee to be understood, map will be on your side\n\nCan every for-loop be replaced by a function and map?\n\nI think yes, but it becomes less practical when an iteration depends on the previous iteration, though I rarely encounter that problem\n\nWhat about the base R functions apply, sapply, lapply?\n\nlapply is very similar to map, though it lacks some very nice features which can be discovered over time. The other apply-functions can give surprising results, except for vapply, which is concise but slightly more complicated.\n\nWhat about map_df, map_lgl, map_dbl and all the other maps?\n\nAll these functions differ in what their output is. However, the list resulting from simple map() can easily be transformed into any of these. After getting to grips with map, all the other maps fall into place.\n\nWhat if I have more than one vector or list as input?\n\nmap2() takes two vectors as input and pmap() takes any number of vectors as input. I’m using pmap() to iterate over rows in a data.frame, but this is stuff for another blogpost I think."
  },
  {
    "objectID": "posts/map/index.html#where-to-go-from-here",
    "href": "posts/map/index.html#where-to-go-from-here",
    "title": "map()-magic",
    "section": "Where to go from here",
    "text": "Where to go from here\n\nJenny Bryan’s purrr tutorials\nHadley Wickham’s “The joy of functional programming”\nR4DS chapter on iterations"
  },
  {
    "objectID": "posts/theme/index.html",
    "href": "posts/theme/index.html",
    "title": "gg themes",
    "section": "",
    "text": "ggplot2 has become one of the most powerful and flexible visualisation tools, with a large community and lots of people working on new extensions every day. A large number of ways to represent data makes it possible to create nearly anything in ggplot2, from great data journalism to beautiful infographics and generative art. No post-processing required anymore.\nThe general look of a ggplot is controlled by a theme. Anyone using ggplot knows that the default grey theme is usually not what you want to show the world. Modifying themes is very flexible, but a little bit complicated. Even after using it for years, I have to google some things every single time. Creating your own theme is a way to give your plots a consistent and personal design, and will save you a lot of time and many lines of code.\n\nThe default ggplot\nLet’s use the data from gapminder to see how a default plot looks like. We first load a few packages and do some data pre-processing.\n\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(dplyr)\nlibrary(wesanderson)\nlibrary(systemfonts)\n# a bit of data processing\ndat &lt;- gapminder %&gt;% \n        group_by(year, continent) %&gt;% \n        summarise(`Life Expectancy` = mean(lifeExp),\n                  Population = sum(as.numeric(pop)), \n                  .groups = 'drop') %&gt;% \n        rename(Year = year, Continent = continent)\n\nHere is a default theme_grey() scatterplot.\n\nggplot(dat, aes(Year, `Life Expectancy`, color = Continent)) +\n      geom_point()\n\n\n\n\n\n\n\n\nThere are a few things I change all the time:\n\nThe background, which I prefer simple plain, or only with x and y axis lines.\nGrid lines: I usually keep only major grid lines (as they are connected to values) or remove them entirely.\nThe spacing between axis, axis-labels and axis-titles.\nThe font.\nFor themes with axis lines, like theme_classic, the line thickness.\n\n\n\nMaking your own theme\nMaking a new theme is quite simple. We (1) create a function which starts with a standard theme, such as theme_minimal and (2) add all the theme aspects which we prefer for our plots. Finally (3), we add some arguments to make changing things easy which we need often, such as axis and grid lines and the text size. Below is the theme I am using, but of course you can change every other theme aspect too (see theme documentation). I’m often using the ‘Avenir Next’ font, which might not be installed on your system. Using ‘sans’ should always work.\n\ntheme_simple &lt;- function(axis_lines = TRUE, \n                         grid_lines = FALSE,     \n                         text_size = 12,       \n                         line_width = 0.2,\n                         # replace with 'sans' if not working\n                         base_family= 'Avenir Next'){ \n        \n    # start with theme_minimal because it is really simple.\n    th &lt;- ggplot2::theme_minimal(base_family = base_family, \n                               base_size = text_size)\n         \n    # remove the grid lines \n    th &lt;- th + theme(panel.grid=element_blank())\n    \n    # if we want axis lines\n    if (axis_lines) {\n      # We add axis lines and give them our preferred thickness\n        th &lt;- th + \n            theme(axis.line = element_line(linewidth = line_width),\n                  axis.ticks = element_line(linewidth = line_width))\n    } \n    # do we want grid lines?\n    if (grid_lines) {\n        th &lt;- th + \n            theme(panel.grid.major = element_line(linewidth = line_width))\n    }\n    \n    # more space for axis text/title and plot title \n    th &lt;- th + theme(\n              axis.text.x=element_text(margin=margin(t=5)),\n              axis.text.y=element_text(margin=margin(r=5)),\n              axis.title.x=element_text(margin=margin(t=10)),\n              axis.title.y=element_text(margin=margin(r=10)),\n              plot.title=element_text(margin=margin(b=10)))\n    \n    return (th)\n}\n\n\n\nAdding theme_simple to the plot.\nNow, we can add theme_simple() to the plot.\n\nggplot(dat, aes(Year, `Life Expectancy`, color = Continent)) +\n    geom_point() +\n    scale_color_manual(values = wes_palette(\"Darjeeling2\")) + \n    theme_simple()\n\n\n\n\n\n\n\n\nSmall tweaks can sometimes make a big aesthetic difference. ggplot comes with a few themes, like theme_classic(), which are sort of close to what I like my plots to be, but are just not quite there. If you feel the same, it’s time to make your own theme.\nLastly, you can put the code for your theme into an R script and save it, for example as theme_simple.R. The next time you make plots, just source the script to load the theme_simple() function. To use it as the default theme, we can use theme_set like so:\n\nsource(\"theme_simple.R\") \n# set theme_simple as default theme\nggplot2::theme_set(theme_simple()) \n\nThat’s it!\nIf you are plotting in base R, you might say: You need a full blog post just to explain how to make ggplot look like base R with a different font! And I can only say: touché, my friend.\n\n\nAppendix: Installing fonts\nFonts can really make a big difference in the visual design of plots. A lot of freely available fonts are on https://fonts.google.com/. On Mac, I just download them, double click and they are installed. Then, we have to make them available in R. The systemfonts package magically finds all installed fonts from different directories.\n\n# install.packages(\"systemfonts\")\nlibrary(systemfonts)\n# which fonts are installed?\n# print only top 5\nsystem_fonts()[1:5, ]\n\n#&gt; # A tibble: 5 × 9\n#&gt;   path                    index name  family style weight width italic monospace\n#&gt;   &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;ord&gt;  &lt;ord&gt; &lt;lgl&gt;  &lt;lgl&gt;    \n#&gt; 1 /System/Library/Assets…     0 Balo… Baloo… Regu… normal norm… FALSE  FALSE    \n#&gt; 2 /System/Library/Assets…     8 Nira… Niram… Light light  norm… FALSE  FALSE    \n#&gt; 3 /System/Library/Assets…     0 Shob… Shobh… Regu… normal norm… FALSE  FALSE    \n#&gt; 4 /System/Library/Fonts/…     1 Telu… Telug… Bold  bold   norm… FALSE  FALSE    \n#&gt; 5 /Users/msto/Library/Fo…     0 JetB… JetBr… Semi… semib… norm… FALSE  TRUE\n\n\nOther options to import fonts are extrafont and showtext.\n\n\npdf-ing\nSometimes, especially for science publications, plots need to be saved as pdfs. With non-standard fonts, this can be problematic, because they have to be embedded, but a little tweak to ggsave() can help here.\n\nggsave(\"plot.pdf\", device = cairo_pdf)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n2025-04-19\n\n\nd/acc bio\n\n\n\n\n2023-04-05\n\n\nhidden info\n\n\n\n\n2023-02-11\n\n\npiping python\n\n\n\n\n2022-11-10\n\n\ngg themes\n\n\n\n\n2022-01-09\n\n\nmap()-magic\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/piping/index.html",
    "href": "posts/piping/index.html",
    "title": "piping in python",
    "section": "",
    "text": "Data wrangling in Python seems clunky. Yet, it doesn’t have to be. Here is how to pipe in Python."
  },
  {
    "objectID": "posts/piping/index.html#the-problem",
    "href": "posts/piping/index.html#the-problem",
    "title": "piping in python",
    "section": "The problem",
    "text": "The problem\nIn R, we process data beautifully with dplyr and %&gt;%:\n\nsuppressMessages(library(dplyr))\niris %&gt;%\n  mutate(sepal_ratio = Sepal.Width / Sepal.Length) %&gt;%\n  filter(sepal_ratio &gt; 0.5) %&gt;%\n  select(Species, sepal_ratio) %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean_sepal_ratio = mean(sepal_ratio))\n\nIn Python’s pandas, most data wrangling code looks much less great, often like this:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\niris = pd.read_csv(url)\n\niris[\"sepal_ratio\"] = iris[\"sepal_width\"] / iris[\"sepal_length\"]\niris = iris[iris[\"sepal_ratio\"] &gt; 0.5]    #filter\niris = iris[[\"species\", \"sepal_ratio\"]]   #select\niris = iris.groupby(\"species\")            #group by\niris = iris.agg({\"sepal_ratio\": \"mean\"})  #summarise\n\nThis is hard to read because there’s lots of repitition."
  },
  {
    "objectID": "posts/piping/index.html#a-new-hope",
    "href": "posts/piping/index.html#a-new-hope",
    "title": "piping in python",
    "section": "A new hope",
    "text": "A new hope\nBut don’t despair! We can pipe in Python, too. Here is how:\n\niris = pd.read_csv(url)\n\n(iris\n  .assign(sepal_ratio = lambda x: x.sepal_width / x.sepal_length)\n  .query(\"sepal_ratio &gt; 0.5\")\n  .loc[:, [\"species\", \"sepal_ratio\"]]\n  .groupby(\"species\")\n  .agg({\"sepal_ratio\": \"mean\"})\n  )\n\n            sepal_ratio\nspecies                \nsetosa         0.684248\nversicolor     0.530953\nvirginica      0.526112\n\n\nFor now, there are only two main principles to remember:\n\nUse . instead of %&gt;% to pipe. Put . at the beginning of the line.\nUse () around the whole expression. Python will complain otherwise.\n\nThere is a pipeable method for most tasks. Sometimes, though, there isn’t, but you can still make it work.\n\nUse lambda to define a function on the fly (like in .assign() above).\nUse .pipe(), which takes a function as an argument and allows you to pipe it.\n\nHere is an example of .pipe():\n\ndef sepal_ratio(df):\n  return df.assign(sepal_ratio = df.sepal_width / df.sepal_length)\n\n(iris\n  .pipe(sepal_ratio)\n  .query(\"sepal_ratio &gt; 0.5\")\n  .loc[:, [\"species\", \"sepal_ratio\"]]\n  .groupby(\"species\")\n  .agg({\"sepal_ratio\": \"mean\"})\n  )\n\n            sepal_ratio\nspecies                \nsetosa         0.684248\nversicolor     0.530953\nvirginica      0.526112\n\n\nThat’s it! Some people don’t like piping because, they say, it’s harder to debug. I like to simply comment out lines, allowing you to run it line by line, which makes it actually very easy to debug. A pipe is also easy to read as it’s basically like a recipe. Start with a dataframe and change stuff step by step.\nHere’s a quick summary over the most common data wrangling tasks and their pipeable methods and functions in dplyr and pandas:\n\n\n\ntask\ndplyr\npandas\n\n\n\n\nfilter rows\nfilter()\ndf.query()\n\n\npick columns\nselect()\ndf.loc[]\n\n\ngroup by\ngroup_by()\ndf.groupby()\n\n\nsummarise\nsummarise()\ndf.agg()\n\n\nmake new variable\nmutate()\ndf.assign()\n\n\njoin dfs\ninner_join()\ndf.merge()\n\n\nsort df\narrange()\ndf.sort_values()\n\n\nrename columns\nrename()\ndf.rename()\n\n\n\n\n\n\n\n\nLast tip: AI is your friend. If you’re stuck, put your pandas code in chatgpt or GitHub Copilot and ask it to re-write code as a pipe. Seems to work pretty well."
  },
  {
    "objectID": "posts/piping/index.html#not-so-secret-bonus-siuba",
    "href": "posts/piping/index.html#not-so-secret-bonus-siuba",
    "title": "piping in python",
    "section": "Not so secret bonus: siuba",
    "text": "Not so secret bonus: siuba\nThere is also the beautiful siuba package. If you come from R, this might be the way to go. But even if not, it’s still less verbose than pandas. Last time I tried it, the package didn’t quite have everything I needed but I think it grew a lot since then. Here is the same pipeline, but with siuba:\n\nfrom siuba import _, mutate, filter, select, group_by, summarize\n\n(iris\n  &gt;&gt; mutate(sepal_ratio = _.sepal_width / _.sepal_length)\n  &gt;&gt; filter(_.sepal_ratio &gt; 0.5)\n  &gt;&gt; select(_.species, _.sepal_ratio)\n  &gt;&gt; group_by(_.species)\n  &gt;&gt; summarize(mean_sepal_ratio = _.sepal_ratio.mean())\n  )"
  },
  {
    "objectID": "posts/info/index.html",
    "href": "posts/info/index.html",
    "title": "hidden info",
    "section": "",
    "text": "“If I am what I have and if what I have is lost, then who am I?”\n- Erich Fromm\nThe future of knowledge is at a crossroads. Before long, most text and code will be written by or with AI models. What was once explicit human reasoning will become information hidden in neural network weights, undecipherable changes in evergrowing matrices. Model outputs will soon become inputs, and the trajectory of knowledge becomes unclear once we outsourced thinking to the machine.\nFor people who code, Stackoverflow is the place where problems are discussed and solutions are found. The community makes sure that precise questions are asked and that answers are ranked, corrected and accepted. More than merely providing solutions, the platform makes the human reasoning process explicit and open. Stackoverflow documents how we, as humans, think about code. But this could soon be over.\nWhy would anyone go through the effort of constructing a minimal reproducible example, formulating a precise question and waiting for hours or days to get an answer, if ChatGPT gives a decent answer straight away and hassle-free? To get some insights into this, I had a look at the number of questions with a Python tag on Stackoverflow over time. While Python questions were increasing :for years, there is a clear drop since ChatGPT was released.\nThe decline isn’t super strong yet, but we can see where this is going. Stackoverflow, an open catalogue of human reasoning, will be replaced by private human-to-AI interactions. Human-readable information becomes hidden in neural network weights. Knowledge-generation is outsourced into AI systems with billions of parameters.\nCode, arguably, is easy to validate. At least we roughly know when it does what we want. This is different in science, where the reasoning process itself is key to the validity of the results. Soon, AI reasoning will permeate the very foundation of science. AI’s are going to plan experiments, collect and analyse data, draw conclusions and write papers. Much of the human input to new knowledge will be evaluating AI outputs rather than reasoning ourselves.\nWhether AI will outperform human reasoning or be subtly wrong more often than humans, we can’t let the reasoning underpinning science be hidden in AI models. To keep the upper hand, we need to transition to a true open source science, where the community has the opportunity to collectively understand and verify scientific progress. The current peer review system, possibly a failed experiment anyway, won’t be able to keep up with a flood of AI-generated papers. Rather than receiving a scientific stamp-of-approval after being reviewed by only two or three peers, scientific papers should be continously open to scrutiny and improvement by the community. For inspiration, we could have a look at Stackoverflow."
  },
  {
    "objectID": "posts/info/index.html#x-graph",
    "href": "posts/info/index.html#x-graph",
    "title": "hidden info",
    "section": ":x graph",
    "text": ":x graph\nNote: the peak in the graph marks the beginning of the COVID-19 lockdown, where people suddenly had more time to code and ask questions. However, the increase in interest declined relatively quickly. The drop following the peak is therefore unusual and can’t really be compared to the drop at the end."
  },
  {
    "objectID": "posts/info/stackoverflow.html",
    "href": "posts/info/stackoverflow.html",
    "title": "Are Python questions on stackoverflow dropping since ChatGPT?",
    "section": "",
    "text": "This notebook\n* fetches the number of stackoverflow questions per month with the Python tag from stackoverflow using their API\n* plots them\n\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.dates import date2num\nfrom plotnine import *\nfrom datetime import datetime, timedelta\n\n\n# Set the API URL for fetching data\nbase_url = \"https://api.stackexchange.com/2.3/questions\"\n\n# Set the parameters\nparams = {\n    \"site\": \"stackoverflow\",\n    \"tagged\": \"python\",\n    \"pagesize\": 1,\n    \"fromdate\": None,\n    \"todate\": None,\n    \"filter\": \"total\",\n}\n\n\n# Get stackoverflow data for the last 96 months\ncurrent_date = datetime.now()\nend_date = current_date.replace(day=1) - timedelta(days=1)\nstart_date = end_date - timedelta(days=96*30)  \n\nmonths = pd.date_range(start=start_date, end=end_date + pd.Timedelta(days=1), freq='MS')\ndata = []\n\n# Fetch the data from the API\nfor i in range(len(months) - 1):\n    params[\"fromdate\"] = int(months[i].timestamp())\n    params[\"todate\"] = int(months[i+1].timestamp())\n    \n    response = requests.get(base_url, params=params)\n    total_questions = response.json()[\"total\"]\n    \n    data.append({\"month\": months[i].strftime(\"%Y-%m\"), \"questions\": total_questions})\n    \ndf = pd.DataFrame(data)\n\nnov_2022_data = df[df[\"month\"] == \"2022-11\"].iloc[0]\nsns.set(style=\"white\")\nplt.rcParams[\"font.family\"] = \"sans-serif\"\nfig, ax = plt.subplots(figsize=(7, 3.5))\nsns.lineplot(\n    x=\"month\", y=\"questions\", data=df, ax=ax, linewidth=2, color=\"#1f77b4\"\n)\nsns.scatterplot(\n    x=\"month\", y=\"questions\", data=df, ax=ax, color=\"#1f77b4\", s=20\n)\nax.axvline(x=nov_2022_data[\"month\"], ymin=0, ymax=1, linestyle=\"--\", color=\"grey\")\nax.annotate(\n    \"ChatGPT\\nrelease\",\n    xy=(nov_2022_data[\"month\"], nov_2022_data[\"questions\"] + 1000),\n    xytext=(5, 30),\n    textcoords=\"offset points\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", color=\"#3B4252\"),\n    color=\"#3B4252\",\n)\n\nnum_labels = 5\nstep = len(df[\"month\"]) // (num_labels - 1)\nxticks = sorted(\n    list(set(df[\"month\"][::step].tolist() + [nov_2022_data[\"month\"]]))[:-1]\n)\nax.set_xticks(xticks)\nplt.xticks(rotation=60, fontsize=12)\nax.set_ylim(5000, 30000)\nax.set_yticks(range(10000, 31000, 10000))\nplt.yticks(fontsize=12)\nax.set_xlabel(\"Month\", fontsize=14, fontweight=\"bold\", labelpad=15)\nax.set_ylabel(\n    \"# of questions with python tag\", fontsize=14, fontweight=\"bold\", labelpad=15\n)\nax.tick_params(axis=\"both\", colors=\"grey\")\nfor spine in [\"bottom\", \"left\"]:\n    ax.spines[spine].set_color(\"grey\")\nax.grid(False)\nsns.despine()\nplt.show()\n\n\n\n\n\n\nThe number of python questions on Stackoverflow. Data: Stackoverflow API.\n\n\n\n\n\n\n#df.to_csv(\"stackoverflow_python_questions.csv\", index=False)"
  }
]