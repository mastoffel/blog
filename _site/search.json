[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I‚Äôm Martin, an independent researcher. At the moment I‚Äôm thinking about the intersection of AI safety and biology funded by an Open Philantropy grant, while building AI agents for a small company on the side. Previously, I led the development of AutoEmulate, an open-source package to emulate physics simulations using machine learning at the Turing Institute; explored parallels between cultural evolution and AI progress as a PIBBSS fellow; studied deleterious mutations in a bronze-age island sheep population; discovered genetic traces of the 19th century overhunting in pinniped populations around the world, consulted on whether racehorse breeding actually works, and built scientific open source software.\nThings I like: trying to write a blog, Pokemon Red, Steel Maces, Open Water Swimming, The Tao of Pooh, cycling against the wind on Snaefellsnes, playing Ukulele with my 1 year old, saying serious things in spontaneous conversations.\nPlease get in touch if you‚Äôd like to chat!"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "AutoEmulate: Speeding up physics simulations using machine learning.\nJOSS (2025) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nrptR: Intra-class coefficients for hierarchical models.\nMethods in Ecology and Evolution (2017) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\npartR2: R2 for individual fixed effects in hierarchical models.\nPeerJ (2021) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\ninbreedR: Identity disequlibria and more.\nMethods in Ecology and Evolution (2016) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nGCalignR: Aligning gas chromatography samples.\nPlos One (2018) | Code\n:ü••"
  },
  {
    "objectID": "projects.html#software",
    "href": "projects.html#software",
    "title": "projects",
    "section": "",
    "text": "AutoEmulate: Speeding up physics simulations using machine learning.\nJOSS (2025) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nrptR: Intra-class coefficients for hierarchical models.\nMethods in Ecology and Evolution (2017) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\npartR2: R2 for individual fixed effects in hierarchical models.\nPeerJ (2021) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\ninbreedR: Identity disequlibria and more.\nMethods in Ecology and Evolution (2016) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nGCalignR: Aligning gas chromatography samples.\nPlos One (2018) | Code\n:ü••"
  },
  {
    "objectID": "projects.html#research",
    "href": "projects.html#research",
    "title": "projects",
    "section": "Research",
    "text": "Research\nselected, full list here.\n\nQuantitative and evolutionary genetics\n\n\n\n\n\n\n\n\n\nExploring lethal mutations and their evolutionary dynamics.\nEvol letters (2024) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nLong runs of homozygosity have a higher mutation load because their haplotypes are younger.\nEvol Letters (2021) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nEffect size and genetic basis of inbreeding depression in the wild.\nNature Comms (2021) | Code\n:ü••\n\n\n\n\n\nMolecular ecology\n\n\n\n\n\n\n\n\n\nEarly life gut microbiota predict extreme sex differences in Elephant Seals.\nMol Ecol (2020) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nIndustrial exploitation brought one-third of pinniped species to the edge of extinction.\nNature Comms (2018) | Code\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nSkin chemicals encode clues to identify offspring, home colony and potential mates in fur seals.\nPNAS (2015) | Code | Article in the Sueddeutsche Zeitung.\n:ü••\n\n\n\n\n\nConservation genetics\n\n\n\n\n\n\n\n\n\nConservation management strategy impacts inbreeding and mutation load in scimitar-horned oryx.\nPNAS (2023) | :ü••\n\n\n\n\n\nBehavioural ecology\n\n\n\n\n\n\n\n\n\nBiased sex ratios and the impact of early survival on female polygyny.\nPNAS (2017) | Code in Supplementary Material\n:ü••\n\n\n\n\n\nResearch consulting\n\n\n\n\n\n\n\n\n\nImpacts of systemic inbreeding in Thoroughbred racehorses.\nProceedings of the Royal Society B (2022)\n:ü••"
  },
  {
    "objectID": "projects.html#side-projects",
    "href": "projects.html#side-projects",
    "title": "projects",
    "section": "Side projects",
    "text": "Side projects\n\n\n\n\n\n\n\n\n\nDetecting artisanal mines in Myanmar from satellites\n:ü••\n\n\n\n\n\n\n\n\n\n\n\n\nCode: Minimal transformer\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode: Neural Vocoder"
  },
  {
    "objectID": "projects.html#x-el",
    "href": "projects.html#x-el",
    "title": "projects",
    "section": ":x el",
    "text": ":x el\nWe scanned the genomes of thousands of wild Soay sheep for embryonic lethal mutations. These mutations prevent an individual from being born, should it be unlucky enough to receive the same genetic variant from both mum and dad. We found a few and wondered how they are maintained in the population. We show that despite their negative effects, these mutations can persist when genetically linked to beneficial variants.\nAuthors | MA Stoffel, SE Johnston, JG Pilkington, JM Pemberton"
  },
  {
    "objectID": "projects.html#x-roh",
    "href": "projects.html#x-roh",
    "title": "projects",
    "section": ":x roh",
    "text": ":x roh\nRuns of homozygosity (ROH) are long stretches of homozygous genotypes, and turn out to be a weirdly insightful feature of the genome. The longer an ROH is, the fewer generations in the past is it‚Äôs common ancestor haplotype. We show using simulation and empirical genome data that such long ROH have a higher density of deleterious mutations, as natural selection had less time to select against them, compared to short ROH. This has been hypothesised before, but never been shown using real fitness data.\nAuthors | MA Stoffel, SE Johnston, JG Pilkington, JM Pemberton"
  },
  {
    "objectID": "projects.html#x-id",
    "href": "projects.html#x-id",
    "title": "projects",
    "section": ":x id",
    "text": ":x id\nWe know that inbreeding is bad for offspring fitness since Darwin or maybe even since biblical times. The phenomenon is called inbreeding depression and is relevant not just for animals but also for humans. Studying a densely pheno- and genotyped wild population of bronze-age sheep, we show that the effects of even slight inbreeding on survival are severe. We uncover some of the underlying genetic mechanisms using a novel type of genome-wide association study to estimate the effects of deleterious mutations across the genome. This is possibly the most extensive study of its kind, outside of humans.\nAuthors | MA Stoffel, SE Johnston, JG Pilkington, JM Pemberton"
  },
  {
    "objectID": "projects.html#x-nes",
    "href": "projects.html#x-nes",
    "title": "projects",
    "section": ":x nes",
    "text": ":x nes\n:Northern elephant seals have the second-largest sexual size dimorphism of any mammal (right after their Southern sister-species). Adult males can be 3-5 times as heavy as females. Why is that? Sexual selection has favored larger males because they are able to defend large harems of females on the beach against competitors.\nIn young elephant seals (pups) you can‚Äôt really spot a difference between males and females yet. However, when measuring their gut microbiome, sampled with a very long cotton swab, we find very strong sex-differences from early on. This opens the possibility for microbes to provide an adaptation to these two very different life-history strategies of female and male elephant seals.\nAuthors | Martin A Stoffel, Karina Acevedo‚ÄêWhitehouse, Nami Morales‚ÄêDur√°n, Stefanie Grosser, Nayden Chakarov, Oliver Kr√ºger, Hazel J Nichols, Fernando R Elorriaga‚ÄêVerplancken, Joseph I Hoffman"
  },
  {
    "objectID": "projects.html#x-bot",
    "href": "projects.html#x-bot",
    "title": "projects",
    "section": ":x bot",
    "text": ":x bot\nThe scale of industrial seal hunting in the 18th-20th century was large, yet somehow overshadowed by the even larger whaling industry. Using genetics and a dataset of more than 11,000 seals, we estimate that many populations were on the edge of extinction. While only two species went extinct so far (the Carribean monk seal and the Japanese sea lion), others have lost most of their diversity.\nAuthors | MA Stoffel, Emily Humble, AJ Paijmans, Karina Acevedo-Whitehouse, Barbara Louise Chilvers, B Dickerson, F Galimberti, Neil J Gemmell, SD Goldsworthy, HJ Nichols, Oliver Kr√ºger, S Negro, A Osborne, T Pastor, Bruce Cameron Robertson, S Sanvito, JK Schultz, ABA Shafer, Jochen BW Wolf, Joseph I Hoffman"
  },
  {
    "objectID": "projects.html#x-plov",
    "href": "projects.html#x-plov",
    "title": "projects",
    "section": ":x plov",
    "text": ":x plov\nAround 60% of adult snowy plovers are male, leading to a mating system where females are polygynous, because they can chose. My friend Luke led this project, where we wanted to know where this sex bias comes from. Turns out, at birth its 50/50, and most of the sex-bias in adults starts in juveniles, where males have lower survival rates than females. We argue that two-sex population models (as used in our study) are underused but essential to understand population dynamics and light on the remaining mysteries around sexual selection.\nAuthors | Luke J Eberhart-Phillips, Clemens K√ºpper, Tom EX Miller, Medardo Cruz-L√≥pez, Kathryn H Maher, Natalie Dos Remedios, Martin A Stoffel, Joseph I Hoffman, Oliver Kr√ºger, Tam√°s Sz√©kely"
  },
  {
    "objectID": "projects.html#x-chem",
    "href": "projects.html#x-chem",
    "title": "projects",
    "section": ":x chem",
    "text": ":x chem\nFur seal mothers have to find their own offspring in dense colonies among thousands of others when they return from their foraging trips at sea. Over distance, calls seem important but at close range sniffing is common. We showed that seal scent glands contain a mix of chemicals, which might be partially determined by genes and which make it possible to identify related individuals. To do this, we developed a new algorithm to work with gas-chromatography data from wild animals (GCalignR, see below) and borrowed analytical methods from psychology.\nAuthors | Martin A Stoffel, Barbara A Caspers, Jaume Forcada, Athina Giannakara, Markus Baier, Luke Eberhart-Phillips, Caroline M√ºller, Joseph I Hoffman"
  },
  {
    "objectID": "projects.html#x-horse",
    "href": "projects.html#x-horse",
    "title": "projects",
    "section": ":x horse",
    "text": ":x horse\nWe show that genomic inbreeding reduces the changes of a Thoroughbred horse ever making it to the racecourse, and pinpoint a genomic region where homozygosity has a particularly large effect, independent of genome-wide inbreeding. Results were a paper, the full reproducible analysis and a patent for the method.\nAuthors | Emmeline W Hill, Martin A Stoffel, Beatrice A McGivney, David E MacHugh, Josephine M Pemberton"
  },
  {
    "objectID": "projects.html#x-oryx",
    "href": "projects.html#x-oryx",
    "title": "projects",
    "section": ":x oryx",
    "text": ":x oryx\nto do\nAuthors | sEmily Humble, Martin A. Stoffel, Kara Dicks, Alex D. Ball, Rebecca M. Gooley, Justin Chuven, Ricardo Pusey, Mohammed Al Remeithi, Klaus-Peter Koepfli, Budhan Pukazhenthi, Helen Senn, Rob Ogden"
  },
  {
    "objectID": "projects.html#x-ae",
    "href": "projects.html#x-ae",
    "title": "projects",
    "section": ":x ae",
    "text": ":x ae\nSimulations of complex systems are slow. To speed them up for research and application, we have to emulate them, often using machine learning. This is difficult though. AutoEmulate‚Äôs goal is to provide a low-code platform to do all this automatically. I‚Äôve created the package and was lead developer during my time at the Turing Institute. On the left is the old hex logo."
  },
  {
    "objectID": "projects.html#x-rp",
    "href": "projects.html#x-rp",
    "title": "projects",
    "section": ":x rp",
    "text": ":x rp\nrptR calculates intra-class coefficients (also called repeatabilities, hence the name) based on generalised linear mixed models. It does just that, but pretty well, and has somehow become a standard in various fields. Hence the 191x field-citation ratio."
  },
  {
    "objectID": "projects.html#x-p2",
    "href": "projects.html#x-p2",
    "title": "projects",
    "section": ":x p2",
    "text": ":x p2\npartR2 uses a few tricks to calculate the explained variance per fixed effect in GLMMs. There‚Äôs not much else doing this properly, so it has become quite popular. But take care: it‚Äôs key to think what exactly it is that you want to know, especally for more complicated models involving interactions etc."
  },
  {
    "objectID": "posts/theme/index.html",
    "href": "posts/theme/index.html",
    "title": "gg themes",
    "section": "",
    "text": "ggplot2 has become one of the most powerful and flexible visualisation tools, with a large community and lots of people working on new extensions every day. A large number of ways to represent data makes it possible to create nearly anything in ggplot2, from great data journalism to beautiful infographics and generative art. No post-processing required anymore.\nThe general look of a ggplot is controlled by a theme. Anyone using ggplot knows that the default grey theme is usually not what you want to show the world. Modifying themes is very flexible, but a little bit complicated. Even after using it for years, I have to google some things every single time. Creating your own theme is a way to give your plots a consistent and personal design, and will save you a lot of time and many lines of code.\n\nThe default ggplot\nLet‚Äôs use the data from gapminder to see how a default plot looks like. We first load a few packages and do some data pre-processing.\n\nlibrary(ggplot2)\nlibrary(gapminder)\nlibrary(dplyr)\nlibrary(wesanderson)\nlibrary(systemfonts)\n# a bit of data processing\ndat &lt;- gapminder %&gt;% \n        group_by(year, continent) %&gt;% \n        summarise(`Life Expectancy` = mean(lifeExp),\n                  Population = sum(as.numeric(pop)), \n                  .groups = 'drop') %&gt;% \n        rename(Year = year, Continent = continent)\n\nHere is a default theme_grey() scatterplot.\n\nggplot(dat, aes(Year, `Life Expectancy`, color = Continent)) +\n      geom_point()\n\n\n\n\n\n\n\n\nThere are a few things I change all the time:\n\nThe background, which I prefer simple plain, or only with x and y axis lines.\nGrid lines: I usually keep only major grid lines (as they are connected to values) or remove them entirely.\nThe spacing between axis, axis-labels and axis-titles.\nThe font.\nFor themes with axis lines, like theme_classic, the line thickness.\n\n\n\nMaking your own theme\nMaking a new theme is quite simple. We (1) create a function which starts with a standard theme, such as theme_minimal and (2) add all the theme aspects which we prefer for our plots. Finally (3), we add some arguments to make changing things easy which we need often, such as axis and grid lines and the text size. Below is the theme I am using, but of course you can change every other theme aspect too (see theme documentation). I‚Äôm often using the ‚ÄòAvenir Next‚Äô font, which might not be installed on your system. Using ‚Äòsans‚Äô should always work.\n\ntheme_simple &lt;- function(axis_lines = TRUE, \n                         grid_lines = FALSE,     \n                         text_size = 12,       \n                         line_width = 0.2,\n                         # replace with 'sans' if not working\n                         base_family= 'Avenir Next'){ \n        \n    # start with theme_minimal because it is really simple.\n    th &lt;- ggplot2::theme_minimal(base_family = base_family, \n                               base_size = text_size)\n         \n    # remove the grid lines \n    th &lt;- th + theme(panel.grid=element_blank())\n    \n    # if we want axis lines\n    if (axis_lines) {\n      # We add axis lines and give them our preferred thickness\n        th &lt;- th + \n            theme(axis.line = element_line(linewidth = line_width),\n                  axis.ticks = element_line(linewidth = line_width))\n    } \n    # do we want grid lines?\n    if (grid_lines) {\n        th &lt;- th + \n            theme(panel.grid.major = element_line(linewidth = line_width))\n    }\n    \n    # more space for axis text/title and plot title \n    th &lt;- th + theme(\n              axis.text.x=element_text(margin=margin(t=5)),\n              axis.text.y=element_text(margin=margin(r=5)),\n              axis.title.x=element_text(margin=margin(t=10)),\n              axis.title.y=element_text(margin=margin(r=10)),\n              plot.title=element_text(margin=margin(b=10)))\n    \n    return (th)\n}\n\n\n\nAdding theme_simple to the plot.\nNow, we can add theme_simple() to the plot.\n\nggplot(dat, aes(Year, `Life Expectancy`, color = Continent)) +\n    geom_point() +\n    scale_color_manual(values = wes_palette(\"Darjeeling2\")) + \n    theme_simple()\n\n\n\n\n\n\n\n\nSmall tweaks can sometimes make a big aesthetic difference. ggplot comes with a few themes, like theme_classic(), which are sort of close to what I like my plots to be, but are just not quite there. If you feel the same, it‚Äôs time to make your own theme.\nLastly, you can put the code for your theme into an R script and save it, for example as theme_simple.R. The next time you make plots, just source the script to load the theme_simple() function. To use it as the default theme, we can use theme_set like so:\n\nsource(\"theme_simple.R\") \n# set theme_simple as default theme\nggplot2::theme_set(theme_simple()) \n\nThat‚Äôs it!\nIf you are plotting in base R, you might say: You need a full blog post just to explain how to make ggplot look like base R with a different font! And I can only say: touch√©, my friend.\n\n\nAppendix: Installing fonts\nFonts can really make a big difference in the visual design of plots. A lot of freely available fonts are on https://fonts.google.com/. On Mac, I just download them, double click and they are installed. Then, we have to make them available in R. The systemfonts package magically finds all installed fonts from different directories.\n\n# install.packages(\"systemfonts\")\nlibrary(systemfonts)\n# which fonts are installed?\n# print only top 5\nsystem_fonts()[1:5, ]\n\n#&gt; # A tibble: 5 √ó 9\n#&gt;   path                    index name  family style weight width italic monospace\n#&gt;   &lt;chr&gt;                   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;ord&gt;  &lt;ord&gt; &lt;lgl&gt;  &lt;lgl&gt;    \n#&gt; 1 /System/Library/Assets‚Ä¶     0 Balo‚Ä¶ Baloo‚Ä¶ Regu‚Ä¶ normal norm‚Ä¶ FALSE  FALSE    \n#&gt; 2 /System/Library/Assets‚Ä¶     8 Nira‚Ä¶ Niram‚Ä¶ Light light  norm‚Ä¶ FALSE  FALSE    \n#&gt; 3 /System/Library/Assets‚Ä¶     0 Shob‚Ä¶ Shobh‚Ä¶ Regu‚Ä¶ normal norm‚Ä¶ FALSE  FALSE    \n#&gt; 4 /System/Library/Fonts/‚Ä¶     1 Telu‚Ä¶ Telug‚Ä¶ Bold  bold   norm‚Ä¶ FALSE  FALSE    \n#&gt; 5 /Users/msto/Library/Fo‚Ä¶     0 JetB‚Ä¶ JetBr‚Ä¶ Semi‚Ä¶ semib‚Ä¶ norm‚Ä¶ FALSE  TRUE\n\n\nOther options to import fonts are extrafont and showtext.\n\n\npdf-ing\nSometimes, especially for science publications, plots need to be saved as pdfs. With non-standard fonts, this can be problematic, because they have to be embedded, but a little tweak to ggsave() can help here.\n\nggsave(\"plot.pdf\", device = cairo_pdf)"
  },
  {
    "objectID": "posts/info/stackoverflow.html",
    "href": "posts/info/stackoverflow.html",
    "title": "Are Python questions on stackoverflow dropping since ChatGPT?",
    "section": "",
    "text": "This notebook\n* fetches the number of stackoverflow questions per month with the Python tag from stackoverflow using their API\n* plots them\n\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.dates import date2num\nfrom plotnine import *\nfrom datetime import datetime, timedelta\n\n\n# Set the API URL for fetching data\nbase_url = \"https://api.stackexchange.com/2.3/questions\"\n\n# Set the parameters\nparams = {\n    \"site\": \"stackoverflow\",\n    \"tagged\": \"python\",\n    \"pagesize\": 1,\n    \"fromdate\": None,\n    \"todate\": None,\n    \"filter\": \"total\",\n}\n\n\n# Get stackoverflow data for the last 96 months\ncurrent_date = datetime.now()\nend_date = current_date.replace(day=1) - timedelta(days=1)\nstart_date = end_date - timedelta(days=96*30)  \n\nmonths = pd.date_range(start=start_date, end=end_date + pd.Timedelta(days=1), freq='MS')\ndata = []\n\n# Fetch the data from the API\nfor i in range(len(months) - 1):\n    params[\"fromdate\"] = int(months[i].timestamp())\n    params[\"todate\"] = int(months[i+1].timestamp())\n    \n    response = requests.get(base_url, params=params)\n    total_questions = response.json()[\"total\"]\n    \n    data.append({\"month\": months[i].strftime(\"%Y-%m\"), \"questions\": total_questions})\n    \ndf = pd.DataFrame(data)\n\nnov_2022_data = df[df[\"month\"] == \"2022-11\"].iloc[0]\nsns.set(style=\"white\")\nplt.rcParams[\"font.family\"] = \"sans-serif\"\nfig, ax = plt.subplots(figsize=(7, 3.5))\nsns.lineplot(\n    x=\"month\", y=\"questions\", data=df, ax=ax, linewidth=2, color=\"#1f77b4\"\n)\nsns.scatterplot(\n    x=\"month\", y=\"questions\", data=df, ax=ax, color=\"#1f77b4\", s=20\n)\nax.axvline(x=nov_2022_data[\"month\"], ymin=0, ymax=1, linestyle=\"--\", color=\"grey\")\nax.annotate(\n    \"ChatGPT\\nrelease\",\n    xy=(nov_2022_data[\"month\"], nov_2022_data[\"questions\"] + 1000),\n    xytext=(5, 30),\n    textcoords=\"offset points\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", color=\"#3B4252\"),\n    color=\"#3B4252\",\n)\n\nnum_labels = 5\nstep = len(df[\"month\"]) // (num_labels - 1)\nxticks = sorted(\n    list(set(df[\"month\"][::step].tolist() + [nov_2022_data[\"month\"]]))[:-1]\n)\nax.set_xticks(xticks)\nplt.xticks(rotation=60, fontsize=12)\nax.set_ylim(5000, 30000)\nax.set_yticks(range(10000, 31000, 10000))\nplt.yticks(fontsize=12)\nax.set_xlabel(\"Month\", fontsize=14, fontweight=\"bold\", labelpad=15)\nax.set_ylabel(\n    \"# of questions with python tag\", fontsize=14, fontweight=\"bold\", labelpad=15\n)\nax.tick_params(axis=\"both\", colors=\"grey\")\nfor spine in [\"bottom\", \"left\"]:\n    ax.spines[spine].set_color(\"grey\")\nax.grid(False)\nsns.despine()\nplt.show()\n\n\n\n\n\n\nThe number of python questions on Stackoverflow. Data: Stackoverflow API.\n\n\n\n\n\n\n#df.to_csv(\"stackoverflow_python_questions.csv\", index=False)"
  },
  {
    "objectID": "posts/smcp/index.html",
    "href": "posts/smcp/index.html",
    "title": "free science",
    "section": "",
    "text": "At first, AIs were created to understand the world. Now, a world is created that AIs can understand better.\nThe Model Context Protocol (MCP) defines a standardised interface between things and AI. Until MCP, LLMs like ChatGPT or Claude had to figure out where to look for data, how to use an application, or how to navigate a website. This often goes wrong, because apps are all different, and data is often not accessible.\nNow, if you‚Äôd like an AI to easily access your (app, system, data), you can create an MCP server. An MCP consists of a few fundamental building blocks like tools, resources and prompts for whethever task it is you‚Äôd like AIs to do. These building blocks are attached to your app and provide exactly the information that an AI needs to use it. Eventually, if everything from browsers to online shopping to booking flights has MCP servers, AIs will be able to easily do all these things for us, because they‚Äôll know how to use them.\nWouldn‚Äôt it be cool if science had MCPs? Say, each paper has its own MCP server that cleanly exposes all important parts, such as methods, conclusions, code and data, independent of the layout of the journal or the structure of the code or data repo? Each paper-MCP would also be registered somewhere, so that AIs can just search for it. Let‚Äôs call this protocol the Science Model Context Protocol (SMCP).\nHere‚Äôs a list of things that a high-bandwidth, high-accuracy AI-science interface through SMCPs would enable:\n\nautomated synthesis: AI agents could reliably synthesise knowledge through systematic-reviews and meta-analyses, effectively enabling anyone to summarise state of the art knowledge on any question. This could dramatically accelerate science and improve/save many lives.\ndecentralised knowledge: tacit knowledge and skills are highly centralised within few institutions. I was a Postdoc at the University of Edinburgh, which is a hub for stats and genetics, making it much easier to produce high quality publications even as a fresh PhD student. What if everyone, no matter their University, could have easy access to the models, code, and rational behind them? This is what SCMPs will do.\nde-duplicating efforts: Too much time is spent replicating code for data-processing and analyses. Through an SCMP, AI can recreate them and adapt them to different use cases, freeing up time and capacity for researchers to explore new things.\nlive science + digital twins: most research is a one-off. Get the data, run the experiment, analyse, publish. However, what if more data comes along? Especially in the context of a research synthesis? SCMPs will facilitate continuous analyses, added more data and updating the results over time. I imagine that many important papers will have live ‚Äúdigital twins‚Äù which incorporate and publish continuous updates.\nstreamlined evaluation: AI agents can review bugs in code, flaws in statistical modelling and experimental design. Humans can think evaluate the bigger picture and conclusions. This wouldn‚Äôt just save time, but upskill humans and AIs in the process.\n\nLet‚Äôs be clear though, there are risks too:\n\nstreamlining scientific information for AIs will speed up AGI timelines. Are we ready for this?\nit also makes it easier for bad actors to access knowledge via AI, e.g.¬†biotech, weapons\n\nWhenever we decentralise information, it comes with benefits and risks. In the age of AI, the trajectory is less clear than ever. Should we free science?"
  },
  {
    "objectID": "posts/piping/index.html",
    "href": "posts/piping/index.html",
    "title": "piping in python",
    "section": "",
    "text": "Data wrangling in Python seems clunky. Yet, it doesn‚Äôt have to be. Here is how to pipe in Python."
  },
  {
    "objectID": "posts/piping/index.html#the-problem",
    "href": "posts/piping/index.html#the-problem",
    "title": "piping in python",
    "section": "The problem",
    "text": "The problem\nIn R, we process data beautifully with dplyr and %&gt;%:\n\nsuppressMessages(library(dplyr))\niris %&gt;%\n  mutate(sepal_ratio = Sepal.Width / Sepal.Length) %&gt;%\n  filter(sepal_ratio &gt; 0.5) %&gt;%\n  select(Species, sepal_ratio) %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean_sepal_ratio = mean(sepal_ratio))\n\nIn Python‚Äôs pandas, most data wrangling code looks much less great, often like this:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\niris = pd.read_csv(url)\n\niris[\"sepal_ratio\"] = iris[\"sepal_width\"] / iris[\"sepal_length\"]\niris = iris[iris[\"sepal_ratio\"] &gt; 0.5]    #filter\niris = iris[[\"species\", \"sepal_ratio\"]]   #select\niris = iris.groupby(\"species\")            #group by\niris = iris.agg({\"sepal_ratio\": \"mean\"})  #summarise\n\nThis is hard to read because there‚Äôs lots of repitition."
  },
  {
    "objectID": "posts/piping/index.html#a-new-hope",
    "href": "posts/piping/index.html#a-new-hope",
    "title": "piping in python",
    "section": "A new hope",
    "text": "A new hope\nBut don‚Äôt despair! We can pipe in Python, too. Here is how:\n\niris = pd.read_csv(url)\n\n(iris\n  .assign(sepal_ratio = lambda x: x.sepal_width / x.sepal_length)\n  .query(\"sepal_ratio &gt; 0.5\")\n  .loc[:, [\"species\", \"sepal_ratio\"]]\n  .groupby(\"species\")\n  .agg({\"sepal_ratio\": \"mean\"})\n  )\n\n            sepal_ratio\nspecies                \nsetosa         0.684248\nversicolor     0.530953\nvirginica      0.526112\n\n\nFor now, there are only two main principles to remember:\n\nUse . instead of %&gt;% to pipe. Put . at the beginning of the line.\nUse () around the whole expression. Python will complain otherwise.\n\nThere is a pipeable method for most tasks. Sometimes, though, there isn‚Äôt, but you can still make it work.\n\nUse lambda to define a function on the fly (like in .assign() above).\nUse .pipe(), which takes a function as an argument and allows you to pipe it.\n\nHere is an example of .pipe():\n\ndef sepal_ratio(df):\n  return df.assign(sepal_ratio = df.sepal_width / df.sepal_length)\n\n(iris\n  .pipe(sepal_ratio)\n  .query(\"sepal_ratio &gt; 0.5\")\n  .loc[:, [\"species\", \"sepal_ratio\"]]\n  .groupby(\"species\")\n  .agg({\"sepal_ratio\": \"mean\"})\n  )\n\n            sepal_ratio\nspecies                \nsetosa         0.684248\nversicolor     0.530953\nvirginica      0.526112\n\n\nThat‚Äôs it! Some people don‚Äôt like piping because, they say, it‚Äôs harder to debug. I like to simply comment out lines, allowing you to run it line by line, which makes it actually very easy to debug. A pipe is also easy to read as it‚Äôs basically like a recipe. Start with a dataframe and change stuff step by step.\nHere‚Äôs a quick summary over the most common data wrangling tasks and their pipeable methods and functions in dplyr and pandas:\n\n\n\ntask\ndplyr\npandas\n\n\n\n\nfilter rows\nfilter()\ndf.query()\n\n\npick columns\nselect()\ndf.loc[]\n\n\ngroup by\ngroup_by()\ndf.groupby()\n\n\nsummarise\nsummarise()\ndf.agg()\n\n\nmake new variable\nmutate()\ndf.assign()\n\n\njoin dfs\ninner_join()\ndf.merge()\n\n\nsort df\narrange()\ndf.sort_values()\n\n\nrename columns\nrename()\ndf.rename()\n\n\n\n\n\n\n\n\nLast tip: AI is your friend. If you‚Äôre stuck, put your pandas code in chatgpt or GitHub Copilot and ask it to re-write code as a pipe. Seems to work pretty well."
  },
  {
    "objectID": "posts/piping/index.html#not-so-secret-bonus-siuba",
    "href": "posts/piping/index.html#not-so-secret-bonus-siuba",
    "title": "piping in python",
    "section": "Not so secret bonus: siuba",
    "text": "Not so secret bonus: siuba\nThere is also the beautiful siuba package. If you come from R, this might be the way to go. But even if not, it‚Äôs still less verbose than pandas. Last time I tried it, the package didn‚Äôt quite have everything I needed but I think it grew a lot since then. Here is the same pipeline, but with siuba:\n\nfrom siuba import _, mutate, filter, select, group_by, summarize\n\n(iris\n  &gt;&gt; mutate(sepal_ratio = _.sepal_width / _.sepal_length)\n  &gt;&gt; filter(_.sepal_ratio &gt; 0.5)\n  &gt;&gt; select(_.species, _.sepal_ratio)\n  &gt;&gt; group_by(_.species)\n  &gt;&gt; summarize(mean_sepal_ratio = _.sepal_ratio.mean())\n  )"
  },
  {
    "objectID": "posts/info/index.html",
    "href": "posts/info/index.html",
    "title": "hidden info",
    "section": "",
    "text": "‚ÄúIf I am what I have and if what I have is lost, then who am I?‚Äù\n- Erich Fromm\nThe future of knowledge is at a crossroads. Before long, most text and code will be written by or with AI models. What was once explicit human reasoning will become information hidden in neural network weights, undecipherable changes in evergrowing matrices. Model outputs will soon become inputs, and the trajectory of knowledge becomes unclear once we outsourced thinking to the machine.\nFor people who code, Stackoverflow is the place where problems are discussed and solutions are found. The community makes sure that precise questions are asked and that answers are ranked, corrected and accepted. More than merely providing solutions, the platform makes the human reasoning process explicit and open. Stackoverflow documents how we, as humans, think about code. But this could soon be over.\nWhy would anyone go through the effort of constructing a minimal reproducible example, formulating a precise question and waiting for hours or days to get an answer, if ChatGPT gives a decent answer straight away and hassle-free? To get some insights into this, I had a look at the number of questions with a Python tag on Stackoverflow over time. While Python questions were increasing :for years, there is a clear drop since ChatGPT was released.\nThe decline isn‚Äôt super strong yet, but we can see where this is going. Stackoverflow, an open catalogue of human reasoning, will be replaced by private human-to-AI interactions. Human-readable information becomes hidden in neural network weights. Knowledge-generation is outsourced into AI systems with billions of parameters.\nCode, arguably, is easy to validate. At least we roughly know when it does what we want. This is different in science, where the reasoning process itself is key to the validity of the results. Soon, AI reasoning will permeate the very foundation of science. AI‚Äôs are going to plan experiments, collect and analyse data, draw conclusions and write papers. Much of the human input to new knowledge will be evaluating AI outputs rather than reasoning ourselves.\nWhether AI will outperform human reasoning or be subtly wrong more often than humans, we can‚Äôt let the reasoning underpinning science be hidden in AI models. To keep the upper hand, we need to transition to a true open source science, where the community has the opportunity to collectively understand and verify scientific progress. The current peer review system, possibly a failed experiment anyway, won‚Äôt be able to keep up with a flood of AI-generated papers. Rather than receiving a scientific stamp-of-approval after being reviewed by only two or three peers, scientific papers should be continously open to scrutiny and improvement by the community. For inspiration, we could have a look at Stackoverflow."
  },
  {
    "objectID": "posts/info/index.html#x-graph",
    "href": "posts/info/index.html#x-graph",
    "title": "hidden info",
    "section": ":x graph",
    "text": ":x graph\nNote: the peak in the graph marks the beginning of the COVID-19 lockdown, where people suddenly had more time to code and ask questions. However, the increase in interest declined relatively quickly. The drop following the peak is therefore unusual and can‚Äôt really be compared to the drop at the end."
  },
  {
    "objectID": "posts/map/index.html",
    "href": "posts/map/index.html",
    "title": "map()-magic",
    "section": "",
    "text": "‚ÄúForm follows function‚Äù - Louis Sullivan\nOne of the major steps in becoming a more effective R programmer for me was to really adopt functional programming. It made my code more readable, less error-prone, faster, and also simply for fun to write.\nFunctional programming is simply about writing code with functions. Instead of repeating the same line of code over and over or using double-nested for loops, we can abstract the essence of what we are doing into functions.\nA function can then be elegantly applied to many inputs. Here, we will do this with purrr::map(), which I‚Äôm using day in and day out and which is a great starting point to dive into the world of functional programming.\nLet‚Äôs go through some of the map()-magic with a minimal workflow to produce clean, robust and fast code. We will do a small :genome-wide association study, an analysis looking at the association between genes and a trait by fitting a model over and over again for every :SNP in the genome.\nHere are some my favorite packages.\nlibrary(purrr) # provides the key function here: map\nlibrary(furrr) # does map in parallel\nlibrary(dplyr) # does all sorts of magic\nlibrary(glue)  # concatenates strings beautifully\nlibrary(broom) # takes a model and returns a tidy data.frame\nLet‚Äôs see whether drinking coffee has a genetic basis. We make up a trait (coffees per day) and 100 SNPs for 100 individuals."
  },
  {
    "objectID": "posts/map/index.html#simulate-data",
    "href": "posts/map/index.html#simulate-data",
    "title": "map()-magic",
    "section": "Simulate data",
    "text": "Simulate data\n\ncoffees   &lt;- sample(1:6, 100, TRUE)\nsnps      &lt;- replicate(100, sample(c(0,1,2), 100, TRUE))\nsnp_names &lt;- paste0(\"snp\", 1:100)\n\ndat &lt;- data.frame(cbind(coffees, snps)) %&gt;% \n            setNames(c(\"coffees\", snp_names))\n            \nhead(dat[1:5, 1:7])\n\n#&gt;   coffees snp1 snp2 snp3 snp4 snp5 snp6\n#&gt; 1       4    0    1    0    0    2    1\n#&gt; 2       1    2    2    0    0    0    0\n#&gt; 3       4    2    0    1    0    2    0\n#&gt; 4       2    1    2    2    0    1    2\n#&gt; 5       3    1    2    2    1    0    1"
  },
  {
    "objectID": "posts/map/index.html#write-a-function",
    "href": "posts/map/index.html#write-a-function",
    "title": "map()-magic",
    "section": "1) Write a function",
    "text": "1) Write a function\nWe could now do 100 linear models manually by writing 100 lines of code, or we could do a for loop.\nInstead, let‚Äôs write a function to fit one model, and then apply this function to each SNP. We generally want the thing that changes (i.e.¬†snp_name) to be the first argument. The function below fits a linear model of coffee consumption with a snp as predictor, and extracts the model estimate and p-value for the SNP. It returns a one-row data.frame. I generally like my functions to return data.frames, because that makes it easy to put together many function outputs into a big data.frame at the end.\n\nfit_model &lt;- function(snp_name, dat) {\n      # write formula using SNP name\n      model_formula &lt;- glue(\"coffees ~ {snp_name}\")\n      # fit linear model\n      fit &lt;- lm(model_formula, data = dat) %&gt;% \n                  broom::tidy() %&gt;%        # tidy results\n                  filter(term == snp_name) # extract snp\n      return(fit)\n      \n}"
  },
  {
    "objectID": "posts/map/index.html#use-map-to-apply-function-to-every-snp",
    "href": "posts/map/index.html#use-map-to-apply-function-to-every-snp",
    "title": "map()-magic",
    "section": "2) Use map() to apply function to every SNP",
    "text": "2) Use map() to apply function to every SNP\nUsing a vector with snp_names, we can apply the function to every SNP. The structure of map() is always the same: map(list/vector, function, additional_arguments). map() always returns a list. We can convert the list to a data.frame with dplyr::bind_rows().\n\n# run gwas\ngwas &lt;- map(snp_names, fit_model, dat) %&gt;% \n              bind_rows()\n# print first three SNPs\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 √ó 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#what-if-something-goes-wrong---map-safely",
    "href": "posts/map/index.html#what-if-something-goes-wrong---map-safely",
    "title": "map()-magic",
    "section": "3) What if something goes wrong? - map() safely",
    "text": "3) What if something goes wrong? - map() safely\nLoops often fail becomes something goes wrong in one or a few iterations.Let‚Äôs introduce a non-existing SNP and try again to see how it fails\n\nsnp_names2 &lt;- c(\"not_a_snp\", snp_names)\n\ngwas &lt;- map(snp_names2, fit_model, dat) %&gt;% \n      bind_rows()\n\n#&gt; Error in `map()`:\n#&gt; ‚Ñπ In index: 1.\n#&gt; Caused by error:\n#&gt; ! object 'not_a_snp' not found\n\n\nWe can make our gwas error-safe using purrr::safely(). This does some magic under the hood which isn‚Äôt so important now. For every iteration, it will return a list with two elements, one for the result and one for the error (equals NULL if there is no error). This way, we always get the results or errors of all our iterations back.\n\nfit_model_safely &lt;- purrr::safely(fit_model)\n\ngwas &lt;- map(snp_names2, fit_model_safely, dat)\ngwas[1:2]\n\n#&gt; [[1]]\n#&gt; [[1]]$result\n#&gt; NULL\n#&gt; \n#&gt; [[1]]$error\n#&gt; &lt;simpleError in eval(predvars, data, env): object 'not_a_snp' not found&gt;\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [[2]]$result\n#&gt; # A tibble: 1 √ó 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1    -0.227     0.211     -1.08   0.285\n#&gt; \n#&gt; [[2]]$error\n#&gt; NULL\n\n\nmap() lets you extract a list element simply by its name. Here, we iterate over the list of results and extract all SNPs that worked.\n\ngwas &lt;- map(gwas, \"result\") %&gt;% \n            bind_rows()\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 √ó 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#what-if-it-takes-too-long---map-in-parallel",
    "href": "posts/map/index.html#what-if-it-takes-too-long---map-in-parallel",
    "title": "map()-magic",
    "section": "4) What if it takes too long? - map() in parallel",
    "text": "4) What if it takes too long? - map() in parallel\nOnce the idea of map() is clear, we can easily parallelise it to run on multiple cores. There is some overhead in collecting computations from several cores so this doesn‚Äôt make much sense when the running time is short. But for longer computations, using 4 cores instead of 1 should make it nearly 4 times as fast.\nWe can use the furrr package here, which mimics purrr functions but can run in parallel. It is based on future, which is why all functions start with future_, for example future_map(). Unlike other ways of parallelising, this approach works on Windows, Mac and Linux. All we have to do now is to first set up a plan() ‚Ä¶\n\n# check available cores\navailableCores()\n# parallelises across 4 cores\nplan(multisession, workers = 4)\n\n‚Ä¶ and then replace map() with future_map().\n\ngwas &lt;- future_map(snp_names, fit_model, dat) %&gt;% \n            bind_rows()\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 √ó 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#the-full-minimal-workflow-for-a-robust-parallel-gwas",
    "href": "posts/map/index.html#the-full-minimal-workflow-for-a-robust-parallel-gwas",
    "title": "map()-magic",
    "section": "The full, minimal workflow for a robust, parallel gwas",
    "text": "The full, minimal workflow for a robust, parallel gwas\n\nplan(multisession, workers = 4)\n\nfit_model &lt;- function(snp_name, dat) {\n      model_formula &lt;- glue(\"coffees ~ {snp_name}\")\n      fit &lt;- lm(model_formula, data = dat) %&gt;% \n                  broom::tidy() %&gt;% \n                  filter(term == snp_name)\n      return(fit)\n}\n\nfit_model_safely &lt;- purrr::safely(fit_model)\ngwas &lt;- future_map(snp_names, fit_model_safely, dat) %&gt;% \n                  map(\"result\") %&gt;% \n                  bind_rows()\n\ngwas[1:3, ]\n\n#&gt; # A tibble: 3 √ó 5\n#&gt;   term  estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 snp1  -0.227       0.211   -1.08     0.285\n#&gt; 2 snp2  -0.00634     0.208   -0.0305   0.976\n#&gt; 3 snp3  -0.00640     0.214   -0.0299   0.976"
  },
  {
    "objectID": "posts/map/index.html#general-thoughts-about-map",
    "href": "posts/map/index.html#general-thoughts-about-map",
    "title": "map()-magic",
    "section": "General thoughts about map",
    "text": "General thoughts about map\nWhen to use map?\n\nWhenever more than two lines of code look similar, whenever a for loop needs two cups of coffee to be understood, map will be on your side\n\nCan every for-loop be replaced by a function and map?\n\nI think yes, but it becomes less practical when an iteration depends on the previous iteration, though I rarely encounter that problem\n\nWhat about the base R functions apply, sapply, lapply?\n\nlapply is very similar to map, though it lacks some very nice features which can be discovered over time. The other apply-functions can give surprising results, except for vapply, which is concise but slightly more complicated.\n\nWhat about map_df, map_lgl, map_dbl and all the other maps?\n\nAll these functions differ in what their output is. However, the list resulting from simple map() can easily be transformed into any of these. After getting to grips with map, all the other maps fall into place.\n\nWhat if I have more than one vector or list as input?\n\nmap2() takes two vectors as input and pmap() takes any number of vectors as input. I‚Äôm using pmap() to iterate over rows in a data.frame, but this is stuff for another blogpost I think."
  },
  {
    "objectID": "posts/map/index.html#where-to-go-from-here",
    "href": "posts/map/index.html#where-to-go-from-here",
    "title": "map()-magic",
    "section": "Where to go from here",
    "text": "Where to go from here\n\nJenny Bryan‚Äôs purrr tutorials\nHadley Wickham‚Äôs ‚ÄúThe joy of functional programming‚Äù\nR4DS chapter on iterations"
  },
  {
    "objectID": "posts/dacc-bio/index.html",
    "href": "posts/dacc-bio/index.html",
    "title": "d/acc bio",
    "section": "",
    "text": "Biological laboratories are less safe than one might think. Looking at known incidents between 2000 and 2021 roughly every couple of weeks a lab worker accidentally gets infected, and roughly once a year, a pathogen escapes the lab. These are only reported accidents, likely to be an underestimate.\nSome of them are hard to believe. The only human infectious disease that we ever managed to eradicate is smallpox, caused by the variola virus. Since its eradication in 1980, there should just be two high-security stashes of it worldwide, one in the US and one in Russia. Yet, a third stash was discovered in 2014, in a six decade old cardboard box in an unsecured storage room on the NIH campus. Just a year later, the Pentagon accidentally sent live Anthrax samples to various places, including South Korea.\nAnd the list goes on. Brucella leaked from a biopharma plant in Lanzhou, infecting 10k people. The 1977 Russian flu pandemic had signs of a lab leak; the underlying H1N1 flu strain resembled a virus circulating 30 years earlier.\nBiosecurity is simply not that easy, humans make mistakes and accidents happen. But accidents are only part of the risk.\nBioweapons have a long history. From catapulting plague-infected bodies to blankets infected with smallpox, humans have found many ways to weaponise biological agents. Several countries had biological weapons programs, and some are suspected to still run them. Yet, the Biological Weapons Convention, an international treaty with the mission to effectively ban the development of bioweapons, runs with a handful of employees on roughly the budget of an average McDonald‚Äôs.\nThen there‚Äôs bioterrorism. In 1984, the Rajneeshee sect contaminated salads in restaurants in Oregon with Salmonella, poisening 750 people. Aum Shinrikyo, a Japanese doomsday cult, tried and failed to develop bioweapons but managed to carry out a chemical weapons attack on the Tokyo subway in 1995, killing and injuring many. In 2001, a US scientist sent anthrax letters to various senators and journals, resulting in several casualties and injuries.\nIn the near future, it will become increasingly possible for terrorists to design much more dangerous bio-agents.\nFirst, biotech is becoming increasingly cheap and available. Sequencing a human genome cost 100 million dollars in 2001, now it‚Äôs a few hundred. We can order synthesised DNA and get it shipped to us in a week. While most companies do screen orders for potentially dangerous sequences, around 20% don‚Äôt. Soon, it might even be possible to just synthesise DNA at home on a small benchtop device. With the right DNA, a real virus can be created using reverse genetics. While this is not that easy, future AI models will be increasingly capable assistants for biotech work, allowing less and less knowledgable actors to create pathogens in the lab.\nSo, the stakes are high. But don‚Äôt despair. In the spirit of d/acc, in the next few posts I‚Äôll have a look at biosecurity technologies that will help safeguard humanity against ever more likely biological threats. d/acc is the idea that we should differentially focus on developing defensive, democratic and decentralised technologies. Think early-detection systems and open source vaccines instead of gain-of-function research. Let‚Äôs have a look what is out there."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n2025-06-25\n\n\nfree science\n\n\n\n\n2025-04-19\n\n\nd/acc bio\n\n\n\n\n2023-04-05\n\n\nhidden info\n\n\n\n\n2023-02-11\n\n\npiping python\n\n\n\n\n2022-11-10\n\n\ngg themes\n\n\n\n\n2022-01-09\n\n\nmap()-magic\n\n\n\n\n\nNo matching items"
  }
]