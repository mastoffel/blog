---
title: "engineered-DNA forensics"
description: "where we are and how to move forward"
# author: "Martin Stoffel"
date: "2025-07-25"
categories: [biosecurity]
title-block-banner: false
draft: true
filters:
    - nutshell
---


Engineered organisms (>bio-agents) are created in labs around the world. Lab-leaks are relatively common even in [:BSL-4](#x-bsl4) labs. Bad actors, including nation states and terror groups have always had a large [interest in bioweapons](../dacc-bio/index.qmd). However, once a bio-agent is out in the wild, it's really difficult to figure out *who* developed it in the first place.

The problem is called **genetic engineering attribution (GEA)**. The challenge of GEA is tracing back *engineered* organisms to their designers through signatures in their DNA. 

But why could that work? There's many degrees of freedom in genetic engineering, and the combination of design, style and tools used creates a unique DNA pattern in the synthetic organism, sometimes leading straight to its lab-of-origin.

GEA is a key defensive technology. Once mature, it will allow us to quickly respond to outbreaks, identify responsible labs to fill safety gaps and, importantly, deter bad actors. It's something we should invest in. 

While the field itself arguably started with the microbial forensics investigations of [Amerithrax](https://www.tandfonline.com/doi/full/10.1080/00396331003612521), GEA back then was manual, specific and slow. In the machine learning era, we're able to develop general tools that work for a wide range of bio-agents and labs. But how far are we?

### 2018: raw nucleotides + CNN

[Nielsen & Voigt, 2018](https://www.nature.com/articles/s41467-018-05378-z) is the first study using machine learning for GEA. They used engineered [:Plasmid](#x-plasmid) sequences from [Addgene](https://www.addgene.org/), an open source database. It's one of the few databases where each engineered sequence is linked to the lab that designed it.

Their data contained 36,764 plasmid sequences from 827 labs. To make the sequences available to model, they simply [:one-hot encoded](#x-one-hot) each nucleotide, resulting in a 16,048 x 4 matrix of for each plasmid sequence. Then, they trained a simple CNN to predict which of the labs the sequence came from. 

```{mermaid}
%%| label: fig-cnn
%%| fig-cap: Nielsen & Voigt CNN Architecture for Lab-of-Origin Prediction

flowchart LR
    A["DNA Sequence<br/>A T G C<br/>1 0 0 0<br/>0 1 0 0<br/>0 0 1 0<br/>0 0 0 1<br/>...<br/>16,048 × 4"] --> B["Conv<br/>+<br/>MaxPool"] --> C["Fully<br/>Connected"] --> D["827 labs<br/>Softmax"]
    
    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    style D fill:#fce4ec,stroke:#c2185b,stroke-width:3px
```

The CNN correctly predicted the lab-of-origin in 48% of held-out sequences, and 70% of the time the correct lab was in the top 10 predictions. This isn't quite good enough to apply to a real case, but they made a point: There actually *is* a unique lab-of-origin signature in these sequences. ML-GEA is up to a promising start.

### 2020: base-pair encoding + LSTM

[Alley et al, 2020](https://www.nature.com/articles/s41467-020-19612-0) are next in line. They also used plasmid data from Addgene. After preprocessing, they had a dataset of over 80,000 sequences from over 1,300 labs, substantially larger than Nielsen & Voigt two years before. 

Instead of directly training a model on nucleotides, they used [:byte-pair encoding (BPE)](#x-bpe) to identify reccuring patterns in the DNA, called motifs. These motifs mapped to codons, regulatory and conserved regions. Each motif then became a token that is fed to the model. 

In addition to DNA, they also trained their model based on six phenotypes such as growth temperature and antibiotic resistance. In a real world scenario, this means that we would have to sequence the sample and run lab tests to get these phenotypes.

To predict the lab-of-origin from sequences and phenotypes, they used an [:LSTM](#x-lstm) trained in a two step process. First, they trained on sequences only, and then added phenotypes and finetuned the model. This training strategy prevented the model to get stuck in a local minimum caused by the phenotype data early in training.

```{mermaid}
%%| label: fig-lstm
%%| fig-cap: Alley et al. deteRNNt architecture for Lab-of-Origin Prediction

flowchart LR
    A["DNA Sequence<br/>ATGCGTAA...<br/>↓<br/>BPE Motifs"] --> B["LSTM"] --> C["Metadata<br/>+<br/>Concat"] --> D["1314 labs<br/>Softmax"]
    
    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:3px
    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:3px
    style D fill:#fce4ec,stroke:#c2185b,stroke-width:3px
```

The sequence+phenotype LSTM predicted the correct lab-of-origin 70% of the time, with a 84% top-10 accuracy. This is better than before, but it's unclear whether the performance increase comes from LSTM vs. CNN, BPE vs raw nucleotide encoding, more sequence data or the addition of meta-data to the training.

The authors also show that the model is well [:calibrated](#x-calibration) and that a simpler random forest model can predict the nation-of-origin with 88% top-3 accuracy compared to 47% for simply guessing the most abundant nations in the database.

### 2022: pangenome + alignment (no deep learning!)

Do we actually need deep learning at all? [Wang et al., 2022](https://www.nature.com/articles/s41467-021-21180-w) don't think so. They developed PlasmidHawk, which uses a much simpler approach based on a [:pangenome](#x-pan), sequence alignment and fragment counting. This is how it works:

*Setup* 

1. create a pangenome from ALL plasmid sequences  
2. align each plasmid sequence back to the pangenome to annote *each fragment or sub-sequence* with lab-of-origin information. These are not unique, some fragments will be annotated with multiple labs. 

*Prediction*  

3. take a new plasmid sequence and align to the pangenome  
4. count how often each lab occurs among the aligned fragments. This is a raw score already. One of the labs will occur most often, making it a good candidate for the lab-of-origin. 
5. but they calculate a lab score - a more elegant, *weighted* version of the raw count. The idea is: the fewer labs share a fragment, the more indicative it is for labs that that have it. 

```{mermaid}
%%| label: fig-plasmidHawk
%%| fig-cap: Wang et al. PlasmidHawk Pipeline for Lab-of-Origin Prediction

flowchart LR
    subgraph S1 ["Setup Phase"]
        E["Addgene<br/>Repository"] 
        F["Build<br/>Pan-genome"] 
        G["Annotate<br/>Lab Origins"]
        E --> F --> G
    end
    
    subgraph S2 ["Prediction Phase"]
        A["Unknown<br/>Plasmid"] 
        B["Align to<br/>Pan-genome"] 
        C["Count<br/>Fragments"] 
        D["Predict<br/>Lab Origin"]
        A --> B --> C --> D
    end
    
    G -.-> B
    
    style S1 fill:none,stroke:none
    style S2 fill:none,stroke:none
    style E fill:#f3e5f5,stroke:#7b1fa2,stroke-width:4px,color:#000
    style F fill:#fff9c4,stroke:#f9a825,stroke-width:4px,color:#000
    style G fill:#fff9c4,stroke:#f9a825,stroke-width:4px,color:#000
    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:4px,color:#000
    style B fill:#fff3e0,stroke:#f57c00,stroke-width:4px,color:#000
    style C fill:#e8f5e8,stroke:#388e3c,stroke-width:4px,color:#000
    style D fill:#fce4ec,stroke:#c2185b,stroke-width:4px,color:#000
```

Their method, called PlasmidHawk, had a 76% accuracy for lab-of-origin prediction and a 85% top-10 accuracy, which is better than the two neural network models before. They say it's more efficient than deep learning too, as updating the pangenome with new plasmids is quick compared to re-training a neural network, but the data isn't that big and DL hardware/software is becoming very efficient, so I believe this isn't a substantial point anymore.

A big advantage of this approach is interpretability. For each novel sequence, the method directly surfaces the association between fragment and lab-of-origin after aligning it to the pangenome. This leaves a few more degrees of freedom though: The size of fragments has to be chosen manually, and so does the similarity threshold for alignment. The method also doesn't allow to incorporate additional meta-data like the LSTM approach above, which will be crucial in real-word GEA.

### 2022: GEA competition

The first [GEA competition](https://www.nature.com/articles/s41467-022-35032-8) had 1,200 competitors to create the best GEA model. They worked with the Alley at al data: 81,833 plasmid sequences, 1314 labs-of-origin to predict. There are many more labs in the Addgene database, but all labs with fewer than ten plasmids were pooled into an "Unknown Engineered" category.

The top-teams' models vastly outperformed previous models, with a top-1 accuracy of 82% and a top-10 accuracy of 95%. 

They were also much better at *negative* attribution - the ability to *exclude* potential designers. This is measured by ab X99 score. X99 is the minimum positive integer N, so that the top-N accuracy is at least 99%. For example, X99 would be 279 if the lab-of-origin would be among the top 279 predicted candidates 99% of the time. In Nilsen & Voigt 2018, the X99 was 898, the winning model in the competition had a score of 299. 

So what were the modeling strategies of the winners? Mostly [:ensembles](#x-ensemble) containing CNNs, though the details (pre-processing etc) varied substantially. 

*Problems*

[Crook et al 2022](https://www.nature.com/articles/s41467-022-35032-8) describe some of the issues they observed in the competition models:

* low [:calibration](#x-calibration) scores, except for the winning model
* problematic large composite "Unknown Engineered" class, in which all labs with fewer than 10 sequences (~2000 labs) are pooled. The top-10 accuracy for the Unknown Engineered class was consistently very high, inflating the overall prediction, thought the problem didn't seem to be very big for the best models.

*deeper dive*

[Soares et al](https://www.nature.com/articles/s43588-022-00234-z) published their solution, which was one of the winners. Their model had a 90% top-10 accuracy.

* circular shift augmentation > reverse-complement augmentation
* BPE
* interpretation via metric learning training for CNNs -> teaches the model to extract embeddings from DNA sequencing while learning embeddings of known labs.
* one-shot learning: predict labs with only one training instance
* identiy important tokens in the sequence
* extract design signatures with integrated gradients
* phenotype: antibiotic resistance, copy number, growth temperature, growth strain, selectable markers and in which species
* metric learning: triplet networks








# thoughts

attribution security benefits:
* inform response (motives? capabilities?)
* identify responsible parties for penalty
* succesful attribution can deter other actors

info for attribution:
* non-technical: location, epidemiological features (middle of conlict? labs nearby? accidental vs purposeful)
* intelligence: whistleblowers, interc. comms, surveillance
* technical forensics: properties of the agent

engineered vs natural
* IARPA FELIX program

security potential of GEA
* avoiding mistaken attribution is key
* deterrence

limits:
* attribution techniques don't detect whether the agent was engineered in the first place (it follows engineering detection)
* risk of false positives
* releases of "non-engineered agents" wouldn't be captured
* designer might not be mis-user
* some actors want to claim that it was them!
* data is from genetic engineers operating "in the clear" 
* sophisticated actors might obfuscate or misdirect attribution,
e.g. use the methodological signature of another actor
    * but these attempts will leave their own signatures

genetic engineering attribution, also:  
* lab-of-origin prediction
* engineered-DNA forensics
* biosec sequence attribution


## :x plasmid

Plasmids are small, circular pieces of DNA that float around outside the actual genome. They are a good vector to transport genes into the cell, are relatively easy to work with and self-replicating. That's why they are often used for genetic engineering. 

## :x bpe

Byte-pair encoding (BPE) is a tokenization method that breaks down DNA sequences into meaningful subunits rather than individual nucleotides. Instead of treating each A, T, G, C separately, BPE identifies frequently occurring pairs and merges them into single tokens, creating a more efficient representation. They also allow inputs to be of different lenghts, which is very useful for DNA sequences.

## :x lstm

Long Short-Term Memory (LSTM) is a type of neural network designed to remember information over long sequences. Unlike regular neural networks that forget previous inputs, LSTMs have special "gates" that control what information to keep, forget, or output. They also take input of different lengths, which is great for DNA sequences.

## :x pan

Traditionally, genomics has worked with a single reference genome against which samples are aligned. This approach loses variation - if a sample has genetic features that can't be aligned to the reference, they're discarded. A pangenome captures all genetic variation across multiple samples in a single data structure, often represented as a graph rather than a linear sequence.

## :x ensemble
Ensemble methods do as they sound: They combine the predictions of multiple different models, often leading to more accurate and reliable results.

## :x calibration
Calibration is about aligning *stated* probabilities with empirical *frequencies*. DL models are often overconfident. When a model is well-calibrated it should do the following: if it predicts something with a 70% probability, the thing should happen 70% of the time. If someone tells me there's a 30% rain probability in Edinburgh today, they I expect that there's rain in 3 out of 10 times they tell me that.

## :x one-hot
One-hot encoding transforms categorical variables into binary vectors where only one element is "hot" (1) and all others are "cold" (0). For DNA nucleotides: A → [1,0,0,0], T → [0,1,0,0], G → [0,0,1,0], C → [0,0,0,1]. This allows machine learning algorithms to process the data.

## :x bsl4
BSL-4 labs have the strictest biosafety precautions, as they deal with agents that are aerosol-transmitted and/or highly dangerous and for which there is often no treatment or vaccine.